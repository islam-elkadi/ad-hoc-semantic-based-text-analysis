{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.summarization.textcleaner import *\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_non_alphanum, strip_multiple_whitespaces, strip_tags, split_alphanum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('./models/model.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pos(data):\n",
    "    \n",
    "    data = remove_stopwords(data)\n",
    "    sentences = [simple_preprocess(sent) for sent in sent_tokenize(data)]\n",
    "    sentences = [[x for x in sent if x in model.vocab] for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    \"\"\"\n",
    "        Removes contractions to clean sentences\n",
    "        \n",
    "        Paras:\n",
    "            raw: raw text data\n",
    "        Returns:\n",
    "            raw: cleaned text\n",
    "    \"\"\"\n",
    "    contractions = { \n",
    "                    \"ain't\": \"is not\",\n",
    "                    \"aren't\": \"are not\",\n",
    "                    \"can't\": \"cannot\",\n",
    "                    \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\",\n",
    "                    \"didn't\": \"did not\",\n",
    "                    \"doesn't\": \"does not\",\n",
    "                    \"don't\": \"do not\",\n",
    "                    \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\",\n",
    "                    \"haven't\": \"have not\",\n",
    "                    \"he'd\": \"he would\",\n",
    "                    \"he'll\": \"he will\",\n",
    "                    \"he's\": \"he is\",\n",
    "                    \"how'd\": \"how did\",\n",
    "                    \"how'll\": \"how will\",\n",
    "                    \"how's\": \"how is\",\n",
    "                    \"i'd\": \"I would\",\n",
    "                    \"i'll\": \"I will\",\n",
    "                    \"i'm\": \"I am\",\n",
    "                    \"i've\": \"I have\",\n",
    "                    \"isn't\": \"is not\",\n",
    "                    \"it'd\": \"it would\",\n",
    "                    \"it'll\": \"it will\",\n",
    "                    \"it's\": \"it is\",\n",
    "                    \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\",\n",
    "                    \"mayn't\": \"may not\",\n",
    "                    \"might've\": \"might have\",\n",
    "                    \"mightn't\": \"might not\",\n",
    "                    \"must've\": \"must have\",\n",
    "                    \"mustn't\": \"must not\",\n",
    "                    \"needn't\": \"need not\",\n",
    "                    \"o'clock\": \"of the clock\",\n",
    "                    \"oughtn't\": \"ought not\",\n",
    "                    \"shan't\": \"shall not\",\n",
    "                    \"sha'n't\": \"shall not\",\n",
    "                    \"she'd\": \"she would\",\n",
    "                    \"she'll\": \"she will\",\n",
    "                    \"she's\": \"she is\",\n",
    "                    \"should've\": \"should have\",\n",
    "                    \"shouldn't\": \"should not\",\n",
    "                    \"shouldn't've\": \"should not have\",\n",
    "                    \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\",\n",
    "                    \"that'd\": \"that would\",\n",
    "                    \"that's\": \"that is\",\n",
    "                    \"there'd\": \"there had\",\n",
    "                    \"there's\": \"there is\",\n",
    "                    \"they'd\": \"they would\",\n",
    "                    \"they'll\": \"they will\",\n",
    "                    \"they're\": \"they are\",\n",
    "                    \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\",\n",
    "                    \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\",\n",
    "                    \"we'll\": \"we will\",\n",
    "                    \"we're\": \"we are\",\n",
    "                    \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\",\n",
    "                    \"what'll\": \"what will\",\n",
    "                    \"what're\": \"what are\",\n",
    "                    \"what's\": \"what is\",\n",
    "                    \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\",\n",
    "                    \"when've\": \"when have\",\n",
    "                    \"where'd\": \"where did\",\n",
    "                    \"where's\": \"where is\",\n",
    "                    \"where've\": \"where have\",\n",
    "                    \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\",\n",
    "                    \"who's\": \"who is\",\n",
    "                    \"who've\": \"who have\",\n",
    "                    \"why's\": \"why has\",\n",
    "                    \"why've\": \"why have\",\n",
    "                    \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\",\n",
    "                    \"would've\": \"would have\",\n",
    "                    \"wouldn't\": \"would not\",\n",
    "                    \"y'all\": \"you all\",\n",
    "                    \"you'd\": \"you had / you would\",\n",
    "                    \"you'll\": \"you will\",\n",
    "                    \"you'll've\": \"you will have\",\n",
    "                    \"you're\": \"you are\",\n",
    "                    \"you've\": \"you have\",\n",
    "                    \"1st\": \"first\",\n",
    "                    \"1 st\": \"first\",\n",
    "                    \"2nd\": \"second\",\n",
    "                    \"2 nd\": \"second\",\n",
    "                    \"3rd\":\"third\",\n",
    "                    \"3 rd\":\"third\",\n",
    "                }\n",
    "    \n",
    "    for contrac in list(contractions.keys()):\n",
    "        text = re.sub(contrac, contractions[contrac], text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class syn_vecs():\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def create_synonyms(self, categories):\n",
    "        synonyms = []\n",
    "        for word in categories:\n",
    "            vecs = model.most_similar(positive = [word])\n",
    "            vecs = [vec[0] for vec in vecs if vec[0] in model.vocab]\n",
    "            vecs.insert(0, word)\n",
    "            synonyms.append(vecs)\n",
    "        return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classify_sentences():\n",
    "\n",
    "    def __init__(self, tokenized_sents, tokens, vecs, model):\n",
    "        self.model = model\n",
    "        self.similarity_mat = np.zeros([len(tokens), len(vecs)])\n",
    "\n",
    "    def similarity_matrix(self, tokenized_sents, synonym_vecs):\n",
    "        for i, token in enumerate(tokenized_sents):\n",
    "            for j, vector in enumerate(synonym_vecs):\n",
    "                self.similarity_mat[i, j] = self.model.n_similarity(vector, token)\n",
    "\n",
    "    def classify_sents(self, tokenized_sents, tokens, synonym_vecs):\n",
    "        classifications = {}\n",
    "        tokenized_sents = np.array(tokenized_sents)\n",
    "        self.similarity_matrix(tokens, synonym_vecs)        \n",
    "        y_pred = np.argmax(self.similarity_mat, axis = 1)\n",
    "\n",
    "        for i, category in enumerate(categories):\n",
    "            idx = np.where(y_pred == i)[0].tolist()\n",
    "            classifications[category] = tokenized_sents[idx].tolist()\n",
    "\n",
    "        return classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"flavor\", \"scent\", \"mental_illness\", \"romance\", \"Len_kutel\"]\n",
    "categories = [x for x in categories if x in model.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"I love eating spicy hand pulled noodles. I also like to buy perfumes. I suffer from clinical depression. But, I really love my wife. I love eating spicy hand pulled noodles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = run_pos(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = sent_tokenize(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = syn_vecs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = vector.create_synonyms(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classify_sentences(tokenized_sents, tokens, synonyms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flavor': ['I love eating spicy hand pulled noodles.',\n",
       "  'I love eating spicy hand pulled noodles.'],\n",
       " 'scent': ['I also like to buy perfumes.'],\n",
       " 'mental_illness': ['I suffer from clinical depression.'],\n",
       " 'romance': ['But,  i really love my wife.']}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify_sents(tokenized_sents, tokens, synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try stuff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    data = strip_tags(data)\n",
    "    data = strip_multiple_whitespaces(data)\n",
    "    data = re.sub(r\"[^\\x00-\\x9f]\", r\"\", data)\n",
    "    data = remove_contractions(data)\n",
    "    data = split_alphanum(data)\n",
    "    data = re.sub(r\"\\.\\.+|\\.+\\,\", r\", \", data)\n",
    "    data = re.sub(r\"\\W+\\)\", r\")\", data)\n",
    "    data = re.sub(r\"\\(\\W+\", r\"(\", data)\n",
    "    data = re.sub(r\"\\s+\\.\", \".\", data)\n",
    "    data = replace_abbreviations(data)\n",
    "    data = \" \".join([sent.capitalize() for sent in split_sentences(data) if len(sent.split()) > 2])\n",
    "    data = \", \".join(data.split(\",\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Friends, stoners, red-eyed countrymen, lend me your ears; for I bring unto thee a tale of the Blue Dream... T’was a calm April night, 2014 it was, and I had eagerly purchased an eighth of some pungent Blue Dream. It’s abundance of sugary trichomes, paired with the thick density of the bud was enough to bring a tear to your eye. I enthusiastically ground up the cheeba, packed a generous bowl and went to town. Eight minutes and a bowl later, I was beginning to assume that my herb wasn’t all that strong…but then it hit me like a 150-ton locomotive of euphoria. “Whoooa” was the only thing that I could say, as I looked at everything around the living room. Everything looked as if it were lagging behind by a few frames, and this cerebral adventure lasted for the first few minutes…but just when I thought that Blue Dream had shown me everything there was to experience about her, her sativa effects began to kick in. All of a sudden, I felt as if I was briskly cruising on a warm cloud, which was followed by an amazing burst of energy. Folks let me tell you, if you’d ever like to find out how an eagle feels when it spreads its majestic wings and takes to the air at 80 mph., this strain is a kickass tool to take you there. Finally, when all of your euphoric energy has been expended, Blue Dream ends her experience with a mellow cruise induced by her indica side. Call in at Jimmy John’s and order 12 sandwiches, fire up Netflix, and take it easy on the couch until you slowly begin to melt into the furniture, because you're going to start to drift off into your happy place; and as soon as you reach that critical point of relaxation, you’re going to sleep like a sloth on twelve doses of Ambien. Folks, I guess the moral of the story here is that Blue Dream is an outstanding and pleasurable strain that is fun for cannabis enthusiasts anywhere on the experience spectrum; from the novice user who is looking to have an easy-going yet memorable experience, to the seasoned smoker who owns a laser pointer and a cat, and anybody in between; but my review alone can’t depict the exquisite effects that Blue Dream has to offer. Roll up a liberal amount of Blue Dream, spark it up, and let her take you on a spectacular trip; you’ll be thankful you did when your mind is blissfully floating through the heavens 15.5 %abd. This is number 1 bs ( khabib time ma man!)  .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
